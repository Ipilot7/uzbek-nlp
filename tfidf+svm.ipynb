{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlHAA8pWom9E"
      },
      "source": [
        "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —É–∑–±–µ–∫—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (0‚Äì5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95HPDvbOpv0S"
      },
      "outputs": [],
      "source": [
        "label2emotion = {\n",
        "    0: \"Sadness (–ì—Ä—É—Å—Ç—å)\",\n",
        "    1: \"Joy (–†–∞–¥–æ—Å—Ç—å)\",\n",
        "    2: \"Love (–õ—é–±–æ–≤—å)\",\n",
        "    3: \"Anger (–ó–ª–æ—Å—Ç—å)\",\n",
        "    4: \"Shame/Fear (–°—Ç—ã–¥/–°—Ç—Ä–∞—Ö)\",\n",
        "    5: \"Surprise (–£–¥–∏–≤–ª–µ–Ω–∏–µ)\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiH-vWg2F0Tm",
        "outputId": "e710af39-cb67-4016-d539-8827e40f2cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install  scikit-learn pandas joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNMDFDRYgMhl",
        "outputId": "f78acd8f-64bc-4750-e2cb-cbdfe7443ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'UzbekLemmatizer'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 54 (delta 19), reused 37 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (54/54), 12.17 KiB | 2.43 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MaksudSharipov/UzbekLemmatizer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YH1GTgjocO6",
        "outputId": "54222ce1-f7cd-452c-adbf-672d5bbb798a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0                     o zimni kamsitilgan his qilmadim\n",
            "1    g amxo rlik qiladigan hushyor lgan odamning li...\n",
            "2    post yozishga daqiqa vaqt ajratyapman o zimni ...\n",
            "3    kamin nostaljik his qilaman mulkda ekanligini ...\n",
            "4                        o zimni g amgin his qilyapman\n",
            "5    oxirgi paytlarda o zimni og ir his qildim bilm...\n",
            "6    tavsiya etilgan miqdorda milligramm marta qabu...\n",
            "7    o zimni o smirlikdagidek chalkash yoshli odamd...\n",
            "8    petronas ko p yillar davomida ishlayman petron...\n",
            "9                       o zimni romantik his qilyapman\n",
            "Name: uzbek_text, dtype: object\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.80      0.72       550\n",
            "           1       0.70      0.80      0.75       704\n",
            "           2       0.75      0.43      0.55       178\n",
            "           3       0.74      0.47      0.57       275\n",
            "           4       0.65      0.54      0.59       212\n",
            "           5       0.76      0.51      0.61        81\n",
            "\n",
            "    accuracy                           0.68      2000\n",
            "   macro avg       0.71      0.59      0.63      2000\n",
            "weighted avg       0.69      0.68      0.68      2000\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['final_svm_model.joblib']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "from UzbekLemmatizer.UzbekLemmatizer import Lemma\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "train_df = pd.read_csv(\"data/raw/train.csv\")\n",
        "val_df = pd.read_csv(\"data/raw/validation.csv\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
        "def load_stopwords(file_path):\n",
        "    stopwords = set()\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if '\\t' in line:\n",
        "                parts = line.split('\\t')\n",
        "                word = parts[-1].strip()\n",
        "            else:\n",
        "                word = line.split('.', 1)[-1].strip()\n",
        "            if word:\n",
        "                stopwords.add(word)\n",
        "    return stopwords\n",
        "# https://zenodo.org/records/5659638\n",
        "uz_stopwords = load_stopwords('data/uzbek_stopwords/stopwords_unigrams.txt')\n",
        "\n",
        "# Stanza (StanfordNLP)\n",
        "# spaCy\n",
        "# Hugging Face Transformers (XLM-RoBERTa, mBERT)\n",
        "\n",
        "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º UzbekLemmatizer\n",
        "def preprocess(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^\\w\\s º ª‚Äô]\", \" \", text) # —É–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
        "    text = re.sub(r\"[ º ª‚Äô]\", \"'\", text) # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã\n",
        "    tokens = text.split()\n",
        "    lemmas = []\n",
        "    for word in tokens:\n",
        "        result = Lemma(word, full=True)\n",
        "        if isinstance(result, list) and len(result) >= 2:\n",
        "            lemma = result[1]\n",
        "            if lemma and lemma not in uz_stopwords:\n",
        "                lemmas.append(lemma)\n",
        "    return \" \".join(lemmas) if lemmas else \"token\"\n",
        "\n",
        "# def preprocess(text):\n",
        "#     text = str(text).lower()\n",
        "#     text = re.sub(r\"[^\\w\\s º ª‚Äô]\", \" \", text)   # —É–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
        "#     text = re.sub(r\"[ º ª‚Äô]\", \"'\", text)        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã\n",
        "#     tokens = text.split()\n",
        "#     tokens = [word for word in tokens if word and word not in uz_stopwords]\n",
        "#     return \" \".join(tokens) if tokens else \"token\"\n",
        "\n",
        "\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "train_df[\"uzbek_text\"] = train_df[\"uzbek_text\"].apply(preprocess)\n",
        "val_df[\"uzbek_text\"] = val_df[\"uzbek_text\"].apply(preprocess)\n",
        "\n",
        "print(train_df[\"uzbek_text\"].head(10))\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"svm\", SVC())\n",
        "])\n",
        "\n",
        "# –ü–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "param_grid = {\n",
        "    \"svm__C\": [0.1, 1, 10],\n",
        "    \"svm__kernel\": [\"linear\", \"rbf\"],\n",
        "    \"svm__gamma\": [\"scale\", \"auto\"]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring=\"accuracy\", verbose=1, n_jobs=-1)\n",
        "grid_search.fit(train_df[\"uzbek_text\"], train_df[\"label\"])\n",
        "\n",
        "# –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å\n",
        "final_model = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"svm\", SVC(**{\n",
        "        \"C\": best_params[\"svm__C\"],\n",
        "        \"kernel\": best_params[\"svm__kernel\"],\n",
        "        \"gamma\": best_params[\"svm__gamma\"]\n",
        "    }))\n",
        "])\n",
        "\n",
        "final_model.fit(train_df[\"uzbek_text\"], train_df[\"label\"])\n",
        "\n",
        "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ validation\n",
        "y_pred = final_model.predict(val_df[\"uzbek_text\"])\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(val_df[\"label\"], y_pred))\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "joblib.dump(final_model, \"final_svm_model.joblib\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHcfIpDT4ETk"
      },
      "source": [
        "–í—ã–≤–æ–¥—ã:\n",
        "–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–∞–ª–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç: –±–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤ —É–º–µ–Ω—å—à–µ–Ω–æ –≤–ª–∏—è–Ω–∏–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–π.\n",
        "\n",
        "–õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –ø–æ –∫–ª–∞—Å—Å–∞–º 0 –∏ 1 (F1 > 0.72), —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –±–æ–ª–µ–µ —á—ë—Ç–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö.\n",
        "\n",
        "–•—É–∂–µ –≤—Å–µ–≥–æ –º–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∫–ª–∞—Å—Å–∞–º–∏ 2 –∏ 3, –≤–µ—Ä–æ—è—Ç–Ω–æ, –∏–∑-–∑–∞ –∏—Ö –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∏–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏.\n",
        "\n",
        "–í —Ü–µ–ª–æ–º, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞, —á—Ç–æ –¥–∞—ë—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è –∑–∞ —Å—á—ë—Ç:\n",
        "\n",
        "—É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—ä—ë–º–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö,\n",
        "\n",
        "—Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
